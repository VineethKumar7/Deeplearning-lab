<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>text_preprocessing_Exercises.ipynb</title>
</head>
<body>
<h1>Text Preprocessing</h1>
<p>We have reviewed and evaluated
statistical tools
and prediction challenges
for sequence data.
Such data can take many forms.
Specifically,
as we will focus on
in many chapters of the book,
text is one of the most popular examples of sequence data.
For example,
an article can be simply viewed as a sequence of words, or even a sequence of characters.
To facilitate our future experiments
with sequence data,
we will dedicate this section
to explain common preprocessing steps for text.
Usually, these steps are:</p>
<ol>
<li>Load text as strings into memory.</li>
<li>Split strings into tokens (e.g., words and characters).</li>
<li>Build a table of vocabulary to map the split tokens to numerical indices.</li>
<li>Convert text into sequences of numerical indices so they can be manipulated by models easily.</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># you need the following line of code only if you use google colab</span>

<span class="err">!</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">d2l</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Collecting</span><span class="w"> </span><span class="n">d2l</span>
<span class="w">  </span><span class="n">Downloading</span><span class="w"> </span><span class="n">d2l</span><span class="o">-</span><span class="mf">0.17</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="n">py3</span><span class="o">-</span><span class="n">none</span><span class="o">-</span><span class="n">any</span><span class="o">.</span><span class="n">whl</span><span class="w"> </span><span class="p">(</span><span class="mi">83</span><span class="w"> </span><span class="n">kB</span><span class="p">)</span>
<span class="err"></span><span class="p">[</span><span class="err">?</span><span class="mi">25</span><span class="n">l</span>
</code></pre></div>

<p>[K     |████                            | 10 kB 19.3 MB/s eta 0:00:01
[K     |███████▉                        | 20 kB 10.6 MB/s eta 0:00:01
[K     |███████████▉                    | 30 kB 8.9 MB/s eta 0:00:01
[K     |███████████████▊                | 40 kB 8.3 MB/s eta 0:00:01
[K     |███████████████████▊            | 51 kB 4.6 MB/s eta 0:00:01
[K     |███████████████████████▋        | 61 kB 4.7 MB/s eta 0:00:01
[K     |███████████████████████████▋    | 71 kB 5.0 MB/s eta 0:00:01
[K     |███████████████████████████████▌| 81 kB 5.6 MB/s eta 0:00:01
[K     |████████████████████████████████| 83 kB 717 kB/s 
    [?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l) (1.1.5)
    Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l) (3.2.2)
    Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l) (2.23.0)
    Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l) (1.19.5)
    Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0)
    Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (4.10.1)
    Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (7.6.5)
    Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.6.1)
    Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.2.0)
    Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.3.1)
    Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.2.0)
    Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.3.5)
    Requirement already satisfied: ipython&gt;=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.5.0)
    Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.1.1)
    Requirement already satisfied: traitlets&gt;=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.1.1)
    Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (4.4.2)
    Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (1.0.18)
    Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (0.7.5)
    Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (4.8.0)
    Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (0.8.1)
    Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (2.6.1)
    Requirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (57.4.0)
    Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (1.15.0)
    Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (0.2.5)
    Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (0.2.0)
    Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (3.5.2)
    Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (1.0.2)
    Requirement already satisfied: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (5.1.3)
    Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;jupyter-&gt;d2l) (4.9.1)
    Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;jupyter-&gt;d2l) (2.6.0)
    Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;d2l) (0.12.1)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;d2l) (2.11.3)
    Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;d2l) (1.8.0)
    Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel-&gt;jupyter-&gt;d2l) (2.8.2)
    Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel-&gt;jupyter-&gt;d2l) (22.3.0)
    Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado&gt;=0.8.1-&gt;notebook-&gt;jupyter-&gt;d2l) (0.7.0)
    Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;notebook-&gt;jupyter-&gt;d2l) (2.0.1)
    Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;d2l) (1.3.2)
    Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;d2l) (3.0.6)
    Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;d2l) (0.11.0)
    Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (1.5.0)
    Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.8.4)
    Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.7.1)
    Requirement already satisfied: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.3)
    Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.5.0)
    Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (4.1.0)
    Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;jupyter-&gt;d2l) (0.5.1)
    Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;jupyter-&gt;d2l) (21.3)
    Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;d2l) (2018.9)
    Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole-&gt;jupyter-&gt;d2l) (1.11.2)
    Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (3.0.4)
    Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (2021.10.8)
    Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (2.10)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (1.24.3)
    Installing collected packages: d2l
    Successfully installed d2l-0.17.0</p>
<h2>Reading the Dataset</h2>
<p>To get started we load text from H. G. Wells' <a href="http://www.gutenberg.org/ebooks/35"><em>The Time Machine</em></a>.
This is a fairly small corpus of just over 30000 words, but for the purpose of what we want to illustrate this is just fine.
More realistic document collections contain many billions of words.
The following function <strong>reads the dataset into a list of text lines</strong>, where each line is a string.
For simplicity, here we ignore punctuation and capitalization.</p>
<div class="codehilite"><pre><span></span><code><span class="n">d2l</span><span class="o">.</span><span class="n">DATA_HUB</span><span class="p">[</span><span class="s1">&#39;time_machine&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">DATA_URL</span> <span class="o">+</span> <span class="s1">&#39;timemachine.txt&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;090b5e7e70c295757f55df93cb0a180b9691891a&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">read_time_machine</span><span class="p">():</span> 
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load the time machine dataset into a list of text lines.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;time_machine&#39;</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;[^A-Za-z]+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>

<span class="n">lines</span> <span class="o">=</span> <span class="n">read_time_machine</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;# text lines: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lines</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lines</span><span class="p">[</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code># text lines: 3221
the time machine by h g wells
twinkled and his usually pale face was flushed and animated the
</code></pre></div>

<h2>Tokenization</h2>
<p>The following <code>tokenize</code> function
takes a list (<code>lines</code>) as the input,
where each element is a text sequence (e.g., a text line).
<strong>Each text sequence is split into a list of tokens</strong>.
A <em>token</em> is the basic unit in text.
In the end,
a list of token lists are returned,
where each token is a string.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="s1">&#39;word&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Split text lines into word or character tokens.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="s1">&#39;word&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">token</span> <span class="o">==</span> <span class="s1">&#39;char&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ERROR: unknown token type: &#39;</span> <span class="o">+</span> <span class="n">token</span><span class="p">)</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">[&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;, &#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;]</span>
<span class="k">[]</span>
<span class="k">[]</span>
<span class="k">[]</span>
<span class="k">[]</span>
<span class="k">[&#39;i&#39;]</span>
<span class="k">[]</span>
<span class="k">[]</span>
<span class="k">[&#39;the&#39;, &#39;time&#39;, &#39;traveller&#39;, &#39;for&#39;, &#39;so&#39;, &#39;it&#39;, &#39;will&#39;, &#39;be&#39;, &#39;convenient&#39;, &#39;to&#39;, &#39;speak&#39;, &#39;of&#39;, &#39;him&#39;]</span>
<span class="k">[&#39;was&#39;, &#39;expounding&#39;, &#39;a&#39;, &#39;recondite&#39;, &#39;matter&#39;, &#39;to&#39;, &#39;us&#39;, &#39;his&#39;, &#39;grey&#39;, &#39;eyes&#39;, &#39;shone&#39;, &#39;and&#39;]</span>
<span class="k">[&#39;twinkled&#39;, &#39;and&#39;, &#39;his&#39;, &#39;usually&#39;, &#39;pale&#39;, &#39;face&#39;, &#39;was&#39;, &#39;flushed&#39;, &#39;and&#39;, &#39;animated&#39;, &#39;the&#39;]</span>
</code></pre></div>

<h2>Vocabulary</h2>
<p>The string type of the token is inconvenient to be used by models, which take numerical inputs.
Now let us <strong>build a dictionary, often called <em>vocabulary</em> as well, to map string tokens into numerical indices starting from 0</strong>.
To do so, we first count the unique tokens in all the documents from the training set,
namely a <em>corpus</em>,
and then assign a numerical index to each unique token according to its frequency.
Rarely appeared tokens are often removed to reduce the complexity.
Any token that does not exist in the corpus or has been removed is mapped into a special unknown token “&lt;unk&gt;”.
We optionally add a list of reserved tokens, such as
“&lt;pad&gt;” for padding,
“&lt;bos&gt;” to present the beginning for a sequence, and “&lt;eos&gt;” for the end of a sequence.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">Vocab</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Vocabulary for text.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">reserved_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">reserved_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Sort according to frequencies</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="n">count_corpus</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_token_freqs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                   <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># The index for the unknown token is 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">reserved_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_to_idx</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">)}</span>
        <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_freqs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">freq</span> <span class="o">&lt;</span> <span class="n">min_freq</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_to_idx</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">token_to_idx</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_to_idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">unk</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># Index for the unknown token</span>
        <span class="k">return</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">token_freqs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># Index for the unknown token</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_freqs</span>

<span class="k">def</span> <span class="nf">count_corpus</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Count token frequencies.&quot;&quot;&quot;</span>
    <span class="c1"># Here `tokens` is a 1D list or 2D list</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="c1"># Flatten a list of token lists into a list of tokens</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">line</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div>

<p>We <strong>construct a vocabulary</strong> using the time machine dataset as the corpus.
Then we print the first few frequent tokens with their indices.</p>
<div class="codehilite"><pre><span></span><code><span class="n">vocab</span> <span class="o">=</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">token_to_idx</span><span class="o">.</span><span class="n">items</span><span class="p">())[:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>[(&#39;&lt;unk&gt;&#39;, 0), (&#39;the&#39;, 1), (&#39;i&#39;, 2), (&#39;and&#39;, 3), (&#39;of&#39;, 4), (&#39;a&#39;, 5), (&#39;to&#39;, 6), (&#39;was&#39;, 7), (&#39;in&#39;, 8), (&#39;that&#39;, 9)]
</code></pre></div>

<p>Now we can <strong>convert each text line into a list of numerical indices</strong>.</p>
<div class="codehilite"><pre><span></span><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;words:&#39;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;indices:&#39;</span><span class="p">,</span> <span class="n">vocab</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>words: [&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;, &#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;]
indices: [1, 19, 50, 40, 2183, 2184, 400]
words: [&#39;twinkled&#39;, &#39;and&#39;, &#39;his&#39;, &#39;usually&#39;, &#39;pale&#39;, &#39;face&#39;, &#39;was&#39;, &#39;flushed&#39;, &#39;and&#39;, &#39;animated&#39;, &#39;the&#39;]
indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]
</code></pre></div>

<h2>Putting All Things Together</h2>
<p>Using the above functions, we <strong>package everything into the <code>load_corpus_time_machine</code> function</strong>, which returns <code>corpus</code>, a list of token indices, and <code>vocab</code>, the vocabulary of the time machine corpus.
The modifications we did here are:
(i) we tokenize text into characters, not words, to simplify the training in later sections;
(ii) <code>corpus</code> is a single list, not a list of token lists, since each text line in the time machine dataset is not necessarily a sentence or a paragraph.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">load_corpus_time_machine</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return token indices and the vocabulary of the time machine dataset.&quot;&quot;&quot;</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">read_time_machine</span><span class="p">()</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="s1">&#39;char&#39;</span><span class="p">)</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="c1"># Since each text line in the time machine dataset is not necessarily a</span>
    <span class="c1"># sentence or a paragraph, flatten all the text lines into a single list</span>
    <span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">line</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">max_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[:</span><span class="n">max_tokens</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">vocab</span>

<span class="n">corpus</span><span class="p">,</span> <span class="n">vocab</span> <span class="o">=</span> <span class="n">load_corpus_time_machine</span><span class="p">()</span>
<span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>(170580, 28)
</code></pre></div>

<h2>Summary</h2>
<ul>
<li>Text is an important form of sequence data.</li>
<li>To preprocess text, we usually split text into tokens, build a vocabulary to map token strings into numerical indices, and convert text data into token indices for  models to manipulate.</li>
</ul>
<h2>Exercises</h2>
<ol>
<li>
<p>Tokenization is a key preprocessing step. It varies for different languages. Try to find another three commonly used methods to tokenize text.</p>
</li>
<li>
<p>In the experiment of this section, tokenize the text into words and vary the <code>min_freq</code> argument of the <code>Vocab</code> instance. How does this affect the vocabulary size?</p>
</li>
</ol>
</body>
</html>