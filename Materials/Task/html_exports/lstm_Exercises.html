<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>lstm_Exercises.ipynb</title>
</head>
<body>
<h1>Long Short-Term Memory (LSTM)</h1>
<p>The challenge to address long-term information preservation and short-term input
skipping in latent variable models has existed for a long time. One of the
earliest approaches to address this was the
long short-term memory (LSTM) <code>Hochreiter.Schmidhuber.1997</code>. It shares many of the properties of the
GRU.
Interestingly, LSTMs have a slightly more complex
design than GRUs but predates GRUs by almost two decades.</p>
<h2>Gated Memory Cell</h2>
<p>Arguably LSTM's design is inspired
by logic gates of a computer.
LSTM introduces a <em>memory cell</em> (or <em>cell</em> for short)
that has the same shape as the hidden state
(some literatures consider the memory cell
as a special type of the hidden state),
engineered to record additional information.
To control the memory cell
we need a number of gates.
One gate is needed to read out the entries from the
cell.
We will refer to this as the
<em>output gate</em>.
A second gate is needed to decide when to read data into the
cell.
We refer to this as the <em>input gate</em>.
Last, we need a mechanism to reset
the content of the cell, governed by a <em>forget gate</em>.
The motivation for such a
design is the same as that of GRUs,
namely to be able to decide when to remember and
when to ignore inputs in the hidden state via a dedicated mechanism. Let us see
how this works in practice.</p>
<h3>Input Gate, Forget Gate, and Output Gate</h3>
<p>Just like in GRUs,
the data feeding into the LSTM gates are
the input at the current time step and
the hidden state of the previous time step,
as illustrated in figure 9.2.1 (https://d2l.ai/chapter_recurrent-modern/lstm.html).
They are processed by
three fully-connected layers with a sigmoid activation function to compute the values of
the input, forget. and output gates.
As a result, values of the three gates
are in the range of $(0, 1)$.</p>
<p>Mathematically,
suppose that there are $h$ hidden units, the batch size is $n$, and the number of inputs is $d$.
Thus, the input is $\mathbf{X}<em>t \in \mathbb{R}^{n \times d}$ and the hidden state of the previous time step is $\mathbf{H}</em>{t-1} \in \mathbb{R}^{n \times h}$. Correspondingly, the gates at time step $t$
are defined as follows: the input gate is $\mathbf{I}_t \in \mathbb{R}^{n \times h}$, the forget gate is $\mathbf{F}_t \in \mathbb{R}^{n \times h}$, and the output gate is $\mathbf{O}_t \in \mathbb{R}^{n \times h}$. They are calculated as follows:</p>
<p>$$
\begin{aligned}
\mathbf{I}<em>t &amp;= \sigma(\mathbf{X}_t \mathbf{W}</em>{xi} + \mathbf{H}<em>{t-1} \mathbf{W}</em>{hi} + \mathbf{b}<em>i),\
\mathbf{F}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}</em>{xf} + \mathbf{H}<em>{t-1} \mathbf{W}</em>{hf} + \mathbf{b}<em>f),\
\mathbf{O}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}</em>{xo} + \mathbf{H}<em>{t-1} \mathbf{W}</em>{ho} + \mathbf{b}_o),
\end{aligned}
$$</p>
<p>where $\mathbf{W}<em>{xi}, \mathbf{W}</em>{xf}, \mathbf{W}<em>{xo} \in \mathbb{R}^{d \times h}$ and $\mathbf{W}</em>{hi}, \mathbf{W}<em>{hf}, \mathbf{W}</em>{ho} \in \mathbb{R}^{h \times h}$ are weight parameters and $\mathbf{b}_i, \mathbf{b}_f, \mathbf{b}_o \in \mathbb{R}^{1 \times h}$ are bias parameters.</p>
<h3>Candidate Memory Cell</h3>
<p>Next we design the memory cell. Since we have not specified the action of the various gates yet, we first introduce the <em>candidate</em> memory cell $\tilde{\mathbf{C}}_t \in \mathbb{R}^{n \times h}$. Its computation is similar to that of the three gates described above, but using a $\tanh$ function with a value range for $(-1, 1)$ as the activation function. This leads to the following equation at time step $t$:</p>
<p>$$\tilde{\mathbf{C}}<em>t = \text{tanh}(\mathbf{X}_t \mathbf{W}</em>{xc} + \mathbf{H}<em>{t-1} \mathbf{W}</em>{hc} + \mathbf{b}_c),$$</p>
<p>where $\mathbf{W}<em>{xc} \in \mathbb{R}^{d \times h}$ and $\mathbf{W}</em>{hc} \in \mathbb{R}^{h \times h}$ are weight parameters and $\mathbf{b}_c \in \mathbb{R}^{1 \times h}$ is a bias parameter.</p>
<p>A quick illustration of the candidate memory cell is shown in figure 9.2.2.</p>
<h3>Memory Cell</h3>
<p>In GRUs, we have a mechanism to govern input and forgetting (or skipping).
Similarly,
in LSTMs we have two dedicated gates for such purposes: the input gate $\mathbf{I}<em>t$ governs how much we take new data into account via $\tilde{\mathbf{C}}_t$ and the forget gate $\mathbf{F}_t$ addresses how much of the old memory cell content $\mathbf{C}</em>{t-1} \in \mathbb{R}^{n \times h}$ we retain. Using the same pointwise multiplication trick as before, we arrive at the following update equation:</p>
<p>$$\mathbf{C}<em>t = \mathbf{F}_t \odot \mathbf{C}</em>{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t.$$</p>
<p>If the forget gate is always approximately 1 and the input gate is always approximately 0, the past memory cells $\mathbf{C}_{t-1}$ will be saved over time and passed to the current time step.
This design is introduced to alleviate the vanishing gradient problem and to better capture
long range dependencies within sequences.</p>
<p>We thus arrive at the flow diagram in figure 9.2.3.</p>
<h3>Hidden State</h3>
<p>Last, we need to define how to compute the hidden state $\mathbf{H}_t \in \mathbb{R}^{n \times h}$. This is where the output gate comes into play. In LSTM it is simply a gated version of the $\tanh$ of the memory cell.
This ensures that the values of $\mathbf{H}_t$ are always in the interval $(-1, 1)$.</p>
<p>$$\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t).$$</p>
<p>Whenever the output gate approximates 1 we effectively pass all memory information through to the predictor, whereas for the output gate close to 0 we retain all the information only within the memory cell and perform no further processing.</p>
<p>Figure 9.2.4 has a graphical illustration of the data flow.</p>
<h2>Implementation from Scratch</h2>
<p>Now let us implement an LSTM from scratch.
As same as the experiments in the notebook about the rnn from the scratch,
we first load the time machine dataset.</p>
<div class="codehilite"><pre><span></span><code><span class="c1">#You need the following line of code only if you use google colab</span>

<span class="err">!</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">d2l</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">dependencies</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">35</span>
<span class="n">train_iter</span><span class="p">,</span> <span class="n">vocab</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">load_data_time_machine</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Collecting</span><span class="w"> </span><span class="n">d2l</span>
<span class="w">  </span><span class="n">Downloading</span><span class="w"> </span><span class="n">d2l</span><span class="o">-</span><span class="mf">0.17</span><span class="o">.</span><span class="mf">0.</span><span class="n">post0</span><span class="o">-</span><span class="n">py3</span><span class="o">-</span><span class="n">none</span><span class="o">-</span><span class="n">any</span><span class="o">.</span><span class="n">whl</span><span class="w"> </span><span class="p">(</span><span class="mi">83</span><span class="w"> </span><span class="n">kB</span><span class="p">)</span>
<span class="err"></span><span class="p">[</span><span class="err">?</span><span class="mi">25</span><span class="n">l</span>
</code></pre></div>

<p>[K     |‚ñà‚ñà‚ñà‚ñà                            | 10 kB 21.8 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 20 kB 9.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 30 kB 8.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 40 kB 7.3 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 51 kB 4.2 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 61 kB 4.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 71 kB 4.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 81 kB 5.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83 kB 443 kB/s 
    [?25hInstalling collected packages: d2l
    Successfully installed d2l-0.17.0.post0
    Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...</p>
<h3><strong>Initializing Model Parameters</strong></h3>
<p>Next we need to define and initialize the model parameters. As previously, the hyperparameter <code>num_hiddens</code> defines the number of hidden units. We initialize weights following a Gaussian distribution with 0.01 standard deviation, and we set the biases to 0.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_lstm_params</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">num_inputs</span> <span class="o">=</span> <span class="n">num_outputs</span> <span class="o">=</span> <span class="n">vocab_size</span>

    <span class="k">def</span> <span class="nf">normal</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>

    <span class="k">def</span> <span class="nf">three</span><span class="p">():</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">normal</span><span class="p">(</span>
            <span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">)),</span> <span class="n">normal</span><span class="p">((</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">)),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>

    <span class="n">W_xi</span><span class="p">,</span> <span class="n">W_hi</span><span class="p">,</span> <span class="n">b_i</span> <span class="o">=</span> <span class="n">three</span><span class="p">()</span>  <span class="c1"># Input gate parameters</span>
    <span class="n">W_xf</span><span class="p">,</span> <span class="n">W_hf</span><span class="p">,</span> <span class="n">b_f</span> <span class="o">=</span> <span class="n">three</span><span class="p">()</span>  <span class="c1"># Forget gate parameters</span>
    <span class="n">W_xo</span><span class="p">,</span> <span class="n">W_ho</span><span class="p">,</span> <span class="n">b_o</span> <span class="o">=</span> <span class="n">three</span><span class="p">()</span>  <span class="c1"># Output gate parameters</span>
    <span class="n">W_xc</span><span class="p">,</span> <span class="n">W_hc</span><span class="p">,</span> <span class="n">b_c</span> <span class="o">=</span> <span class="n">three</span><span class="p">()</span>  <span class="c1"># Candidate memory cell parameters</span>
    <span class="c1"># Output layer parameters</span>
    <span class="n">W_hq</span> <span class="o">=</span> <span class="n">normal</span><span class="p">((</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">))</span>
    <span class="n">b_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Attach gradients</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">W_xi</span><span class="p">,</span> <span class="n">W_hi</span><span class="p">,</span> <span class="n">b_i</span><span class="p">,</span> <span class="n">W_xf</span><span class="p">,</span> <span class="n">W_hf</span><span class="p">,</span> <span class="n">b_f</span><span class="p">,</span> <span class="n">W_xo</span><span class="p">,</span> <span class="n">W_ho</span><span class="p">,</span> <span class="n">b_o</span><span class="p">,</span> <span class="n">W_xc</span><span class="p">,</span> <span class="n">W_hc</span><span class="p">,</span> <span class="n">b_c</span><span class="p">,</span>
        <span class="n">W_hq</span><span class="p">,</span> <span class="n">b_q</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">params</span>
</code></pre></div>

<h3>Defining the Model</h3>
<p>In <strong>the initialization function</strong>, the hidden state of the LSTM needs to return an <em>additional</em> memory cell with a value of 0 and a shape of (batch size, number of hidden units). Hence we get the following state initialization.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">init_lstm_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</code></pre></div>

<p><strong>The actual model</strong> is defined just like what we discussed before: providing three gates and an auxiliary memory cell. Note that only the hidden state is passed to the output layer. The memory cell $\mathbf{C}_t$ does not directly participate in the output computation.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">lstm</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="p">[</span>
        <span class="n">W_xi</span><span class="p">,</span> <span class="n">W_hi</span><span class="p">,</span> <span class="n">b_i</span><span class="p">,</span> <span class="n">W_xf</span><span class="p">,</span> <span class="n">W_hf</span><span class="p">,</span> <span class="n">b_f</span><span class="p">,</span> <span class="n">W_xo</span><span class="p">,</span> <span class="n">W_ho</span><span class="p">,</span> <span class="n">b_o</span><span class="p">,</span> <span class="n">W_xc</span><span class="p">,</span> <span class="n">W_hc</span><span class="p">,</span> <span class="n">b_c</span><span class="p">,</span>
        <span class="n">W_hq</span><span class="p">,</span> <span class="n">b_q</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span>
    <span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="o">=</span> <span class="n">state</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">((</span><span class="n">X</span> <span class="o">@</span> <span class="n">W_xi</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">@</span> <span class="n">W_hi</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_i</span><span class="p">)</span>
        <span class="n">F</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">((</span><span class="n">X</span> <span class="o">@</span> <span class="n">W_xf</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">@</span> <span class="n">W_hf</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_f</span><span class="p">)</span>
        <span class="n">O</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">((</span><span class="n">X</span> <span class="o">@</span> <span class="n">W_xo</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">@</span> <span class="n">W_ho</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_o</span><span class="p">)</span>
        <span class="n">C_tilda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">((</span><span class="n">X</span> <span class="o">@</span> <span class="n">W_xc</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">@</span> <span class="n">W_hc</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_c</span><span class="p">)</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">F</span> <span class="o">*</span> <span class="n">C</span> <span class="o">+</span> <span class="n">I</span> <span class="o">*</span> <span class="n">C_tilda</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">O</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="p">(</span><span class="n">H</span> <span class="o">@</span> <span class="n">W_hq</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_q</span>
        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</code></pre></div>

<h3><strong>Training</strong> and Prediction</h3>
<p>Let us train an LSTM as same as what we did in the notebook about gr, by instantiating the <code>RNNModelScratch</code> class as introduced in the notebook about the RNN from the scratch.</p>
<div class="codehilite"><pre><span></span><code><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="mi">256</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">()</span>
<span class="n">num_epochs</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">RNNModelScratch</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">get_lstm_params</span><span class="p">,</span>
                            <span class="n">init_lstm_state</span><span class="p">,</span> <span class="n">lstm</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">train_ch8</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nv">perplexity</span><span class="w"> </span><span class="mi">1</span>.<span class="mi">1</span>,<span class="w"> </span><span class="mi">17371</span>.<span class="mi">9</span><span class="w"> </span><span class="nv">tokens</span><span class="o">/</span><span class="nv">sec</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">cpu</span>
<span class="nv">time</span><span class="w"> </span><span class="nv">traveller</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">so</span><span class="w"> </span><span class="nv">it</span><span class="w"> </span><span class="nv">will</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">convenient</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">speak</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">himwas</span><span class="w"> </span><span class="nv">e</span>
<span class="nv">traveller</span><span class="w"> </span><span class="nv">thin</span><span class="w"> </span><span class="nv">said</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">time</span><span class="w"> </span><span class="nv">travellerit</span><span class="w"> </span><span class="nv">s</span><span class="w"> </span><span class="nv">against</span><span class="w"> </span><span class="nv">reason</span><span class="w"> </span><span class="nv">s</span>
</code></pre></div>

<p><img alt="svg" src="output_9_1.svg" /></p>
<h2><strong>Concise Implementation</strong></h2>
<p>Using high-level APIs,
we can directly instantiate an <code>LSTM</code> model.
This encapsulates all the configuration details that we made explicit above. The code is significantly faster as it uses compiled operators rather than Python for many details that we spelled out in detail before.</p>
<div class="codehilite"><pre><span></span><code><span class="n">num_inputs</span> <span class="o">=</span> <span class="n">vocab_size</span>
<span class="n">lstm_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">RNNModel</span><span class="p">(</span><span class="n">lstm_layer</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">train_ch8</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nv">perplexity</span><span class="w"> </span><span class="mi">1</span>.<span class="mi">0</span>,<span class="w"> </span><span class="mi">16663</span>.<span class="mi">8</span><span class="w"> </span><span class="nv">tokens</span><span class="o">/</span><span class="nv">sec</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">cpu</span>
<span class="nv">time</span><span class="w"> </span><span class="nv">travelleryou</span><span class="w"> </span><span class="nv">can</span><span class="w"> </span><span class="k">show</span><span class="w"> </span><span class="nv">black</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">white</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">argument</span><span class="w"> </span><span class="nv">said</span><span class="w"> </span><span class="nv">filby</span>
<span class="nv">traveller</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">slight</span><span class="w"> </span><span class="nv">accession</span><span class="w"> </span><span class="nv">ofcheerfulness</span><span class="w"> </span><span class="nv">really</span><span class="w"> </span><span class="nv">thi</span>
</code></pre></div>

<p><img alt="svg" src="output_11_1.svg" /></p>
<p>LSTMs are the prototypical latent variable autoregressive model with nontrivial state control.
Many variants thereof have been proposed over the years, e.g., multiple layers, residual connections, different types of regularization. However, training LSTMs and other sequence models (such as GRUs) are quite costly due to the long range dependency of the sequence.
Later we will encounter alternative models such as transformers that can be used in some cases.</p>
<h2>Summary</h2>
<ul>
<li>LSTMs have three types of gates: input gates, forget gates, and output gates that control the flow of information.</li>
<li>The hidden layer output of LSTM includes the hidden state and the memory cell. Only the hidden state is passed into the output layer. The memory cell is entirely internal.</li>
<li>LSTMs can alleviate vanishing and exploding gradients.</li>
</ul>
<h2>Exercises</h2>
<ol>
<li>
<p>Compare the computational cost and the performance of GRUs, LSTMs, and regular RNNs for a given hidden dimension (use the concise implementation. We use in all of our examples a hidden dimension of 256)</p>
</li>
<li>
<p>Since the candidate memory cell ensures that the value range is between $-1$ and $1$ by  using the $\tanh$ function, why does the hidden state need to use the $\tanh$ function again to ensure that the output value range is between $-1$ and $1$?</p>
</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/1057">Discussions</a></p>
</body>
</html>