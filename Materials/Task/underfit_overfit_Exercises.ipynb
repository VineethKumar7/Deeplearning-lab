{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yb1DwrVFhBa",
    "origin_pos": 0
   },
   "source": [
    "# Model Selection, Underfitting, and Overfitting\n",
    "\n",
    "\n",
    "As machine learning scientists,\n",
    "our goal is to discover *patterns*.\n",
    "But how can we be sure that we have\n",
    "truly discovered a *general* pattern\n",
    "and not simply memorized our data?\n",
    "For example, imagine that we wanted to hunt\n",
    "for patterns among genetic markers\n",
    "linking patients to their dementia status,\n",
    "where the labels are drawn from the set\n",
    "$\\{\\text{dementia}, \\text{mild cognitive impairment}, \\text{healthy}\\}$.\n",
    "Because each person's genes identify them uniquely\n",
    "(ignoring identical siblings),\n",
    "it is possible to memorize the entire dataset.\n",
    "\n",
    "We do not want our model to say\n",
    "*\"That's Bob! I remember him! He has dementia!\"*\n",
    "The reason why is simple.\n",
    "When we deploy the model in the future,\n",
    "we will encounter patients\n",
    "that the model has never seen before.\n",
    "Our predictions will only be useful\n",
    "if our model has truly discovered a *general* pattern.\n",
    "\n",
    "To recapitulate more formally,\n",
    "our goal is to discover patterns\n",
    "that capture regularities in the underlying population\n",
    "from which our training set was drawn.\n",
    "If we are successful in this endeavor,\n",
    "then we could successfully assess risk\n",
    "even for individuals that we have never encountered before.\n",
    "This problem---how to discover patterns that *generalize*---is\n",
    "the fundamental problem of machine learning.\n",
    "\n",
    "The danger is that when we train models,\n",
    "we access just a small sample of data.\n",
    "The largest public image datasets contain\n",
    "roughly one million images.\n",
    "More often, we must learn from only thousands\n",
    "or tens of thousands of data examples.\n",
    "In a large hospital system, we might access\n",
    "hundreds of thousands of medical records.\n",
    "When working with finite samples, we run the risk\n",
    "that we might discover apparent associations\n",
    "that turn out not to hold up when we collect more data.\n",
    "\n",
    "The phenomenon of fitting our training data\n",
    "more closely than we fit the underlying distribution is called *overfitting*, and the techniques used to combat overfitting are called *regularization*.\n",
    "In the previous sections, you might have observed\n",
    "this effect while experimenting with the Fashion-MNIST dataset.\n",
    "If you altered the model structure or the hyperparameters during the experiment, you might have noticed that with enough neurons, layers, and training epochs, the model can eventually reach perfect accuracy on the training set, even as the accuracy on test data deteriorates.\n",
    "\n",
    "\n",
    "## Training Error and Generalization Error\n",
    "\n",
    "In order to discuss this phenomenon more formally,\n",
    "we need to differentiate between training error and generalization error.\n",
    "The *training error* is the error of our model\n",
    "as calculated on the training dataset,\n",
    "while *generalization error* is the expectation of our model's error\n",
    "were we to apply it to an infinite stream of additional data examples\n",
    "drawn from the same underlying data distribution as our original sample.\n",
    "\n",
    "Problematically, we can never calculate the generalization error exactly.\n",
    "That is because the stream of infinite data is an imaginary object.\n",
    "In practice, we must *estimate* the generalization error\n",
    "by applying our model to an independent test set\n",
    "constituted of a random selection of data examples\n",
    "that were withheld from our training set.\n",
    "\n",
    "The following three thought experiments\n",
    "will help illustrate this situation better.\n",
    "Consider a college student trying to prepare for his final exam.\n",
    "A diligent student will strive to practice well\n",
    "and test his abilities using exams from previous years.\n",
    "Nonetheless, doing well on past exams is no guarantee\n",
    "that he will excel when it matters.\n",
    "For instance, the student might try to prepare\n",
    "by rote learning the answers to the exam questions.\n",
    "This requires the student to memorize many things.\n",
    "She might even remember the answers for past exams perfectly.\n",
    "Another student might prepare by trying to understand\n",
    "the reasons for giving certain answers.\n",
    "In most cases, the latter student will do much better.\n",
    "\n",
    "Likewise, consider a model that simply uses a lookup table to answer questions. If the set of allowable inputs is discrete and reasonably small, then perhaps after viewing *many* training examples, this approach would perform well. Still this model has no ability to do better than random guessing when faced with examples that it has never seen before.\n",
    "In reality the input spaces are far too large to memorize the answers corresponding to every conceivable input. For example, consider the black and white $28\\times28$ images. If each pixel can take one among $256$ grayscale values, then there are $256^{784}$ possible images. That means that there are far more low-resolution grayscale thumbnail-sized images than there are atoms in the universe. Even if we could encounter such data, we could never afford to store the lookup table.\n",
    "\n",
    "Last, consider the problem of trying\n",
    "to classify the outcomes of coin tosses (class 0: heads, class 1: tails)\n",
    "based on some contextual features that might be available.\n",
    "Suppose that the coin is fair.\n",
    "No matter what algorithm we come up with,\n",
    "the generalization error will always be $\\frac{1}{2}$.\n",
    "However, for most algorithms,\n",
    "we should expect our training error to be considerably lower,\n",
    "depending on the luck of the draw,\n",
    "even if we did not have any features!\n",
    "Consider the dataset {0, 1, 1, 1, 0, 1}.\n",
    "Our feature-less algorithm would have to fall back on always predicting\n",
    "the *majority class*, which appears from our limited sample to be *1*.\n",
    "In this case, the model that always predicts class 1\n",
    "will incur an error of $\\frac{1}{3}$,\n",
    "considerably better than our generalization error.\n",
    "As we increase the amount of data,\n",
    "the probability that the fraction of heads\n",
    "will deviate significantly from $\\frac{1}{2}$ diminishes,\n",
    "and our training error would come to match the generalization error.\n",
    "\n",
    "### Statistical Learning Theory\n",
    "\n",
    "Since generalization is the fundamental problem in machine learning,\n",
    "you might not be surprised to learn\n",
    "that many mathematicians and theorists have dedicated their lives\n",
    "to developing formal theories to describe this phenomenon.\n",
    "In their [eponymous theorem](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem), Glivenko and Cantelli\n",
    "derived the rate at which the training error\n",
    "converges to the generalization error.\n",
    "In a series of seminal papers, [Vapnik and Chervonenkis](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory)\n",
    "extended this theory to more general classes of functions.\n",
    "This work laid the foundations of statistical learning theory.\n",
    "\n",
    "\n",
    "In the standard supervised learning setting, which we have addressed up until now and will stick with throughout most of this book,\n",
    "we assume that both the training data and the test data\n",
    "are drawn *independently* from *identical* distributions.\n",
    "This is commonly called the *i.i.d. assumption*,\n",
    "which means that the process that samples our data has no memory.\n",
    "In other words,\n",
    "the second example drawn and the third drawn\n",
    "are no more correlated than the second and the two-millionth sample drawn.\n",
    "\n",
    "Being a good machine learning scientist requires thinking critically,\n",
    "and already you should be poking holes in this assumption,\n",
    "coming up with common cases where the assumption fails.\n",
    "What if we train a mortality risk predictor\n",
    "on data collected from patients at UCSF Medical Center,\n",
    "and apply it on patients at Massachusetts General Hospital?\n",
    "These distributions are simply not identical.\n",
    "Moreover, draws might be correlated in time.\n",
    "What if we are classifying the topics of Tweets?\n",
    "The news cycle would create temporal dependencies\n",
    "in the topics being discussed, violating any assumptions of independence.\n",
    "\n",
    "Sometimes we can get away with minor violations of the i.i.d. assumption\n",
    "and our models will continue to work remarkably well.\n",
    "After all, nearly every real-world application\n",
    "involves at least some minor violation of the i.i.d. assumption,\n",
    "and yet we have many useful tools for\n",
    "various applications such as\n",
    "face recognition,\n",
    "speech recognition, and language translation.\n",
    "\n",
    "Other violations are sure to cause trouble.\n",
    "Imagine, for example, if we try to train\n",
    "a face recognition system by training it\n",
    "exclusively on university students\n",
    "and then want to deploy it as a tool\n",
    "for monitoring geriatrics in a nursing home population.\n",
    "This is unlikely to work well since college students\n",
    "tend to look considerably different from the elderly.\n",
    "\n",
    "In subsequent chapters, we will discuss problems\n",
    "arising from violations of the i.i.d. assumption.\n",
    "For now, even taking the i.i.d. assumption for granted,\n",
    "understanding generalization is a formidable problem.\n",
    "Moreover, elucidating the precise theoretical foundations\n",
    "that might explain why deep neural networks generalize as well as they do\n",
    "continues to vex the greatest minds in learning theory.\n",
    "\n",
    "When we train our models, we attempt to search for a function\n",
    "that fits the training data as well as possible.\n",
    "If the function is so flexible that it can catch on to spurious patterns\n",
    "just as easily as to true associations,\n",
    "then it might perform *too well* without producing a model\n",
    "that generalizes well to unseen data.\n",
    "This is precisely what we want to avoid or at least control.\n",
    "Many of the techniques in deep learning are heuristics and tricks\n",
    "aimed at guarding against overfitting.\n",
    "\n",
    "### Model Complexity\n",
    "\n",
    "When we have simple models and abundant data,\n",
    "we expect the generalization error to resemble the training error.\n",
    "When we work with more complex models and fewer examples,\n",
    "we expect the training error to go down but the generalization gap to grow.\n",
    "What precisely constitutes model complexity is a complex matter.\n",
    "Many factors govern whether a model will generalize well.\n",
    "For example a model with more parameters might be considered more complex.\n",
    "A model whose parameters can take a wider range of values\n",
    "might be more complex.\n",
    "Often with neural networks, we think of a model\n",
    "that takes more training iterations as more complex,\n",
    "and one subject to *early stopping* (fewer training iterations) as less complex.\n",
    "\n",
    "It can be difficult to compare the complexity among members\n",
    "of substantially different model classes\n",
    "(say, decision trees vs. neural networks).\n",
    "For now, a simple rule of thumb is quite useful:\n",
    "a model that can readily explain arbitrary facts\n",
    "is what statisticians view as complex,\n",
    "whereas one that has only a limited expressive power\n",
    "but still manages to explain the data well\n",
    "is probably closer to the truth.\n",
    "In philosophy, this is closely related to Popper's\n",
    "criterion of falsifiability\n",
    "of a scientific theory: a theory is good if it fits data\n",
    "and if there are specific tests that can be used to disprove it.\n",
    "This is important since all statistical estimation is\n",
    "*post hoc*,\n",
    "i.e., we estimate after we observe the facts,\n",
    "hence vulnerable to the associated fallacy.\n",
    "For now, we will put the philosophy aside and stick to more tangible issues.\n",
    "\n",
    "In this section, to give you some intuition,\n",
    "we will focus on a few factors that tend\n",
    "to influence the generalizability of a model class:\n",
    "\n",
    "1. The number of tunable parameters. When the number of tunable parameters, sometimes called the *degrees of freedom*, is large, models tend to be more susceptible to overfitting.\n",
    "1. The values taken by the parameters. When weights can take a wider range of values, models can be more susceptible to overfitting.\n",
    "1. The number of training examples. It is trivially easy to overfit a dataset containing only one or two examples even if your model is simple. But overfitting a dataset with millions of examples requires an extremely flexible model.\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "In machine learning, we usually select our final model\n",
    "after evaluating several candidate models.\n",
    "This process is called *model selection*.\n",
    "Sometimes the models subject to comparison\n",
    "are fundamentally different in nature\n",
    "(say, decision trees vs. linear models).\n",
    "At other times, we are comparing\n",
    "members of the same class of models\n",
    "that have been trained with different hyperparameter settings.\n",
    "\n",
    "With MLPs, for example,\n",
    "we may wish to compare models with\n",
    "different numbers of hidden layers,\n",
    "different numbers of hidden units,\n",
    "and various choices of the activation functions\n",
    "applied to each hidden layer.\n",
    "In order to determine the best among our candidate models,\n",
    "we will typically employ a validation dataset.\n",
    "\n",
    "\n",
    "### Validation Dataset\n",
    "\n",
    "In principle we should not touch our test set\n",
    "until after we have chosen all our hyperparameters.\n",
    "Were we to use the test data in the model selection process,\n",
    "there is a risk that we might overfit the test data.\n",
    "Then we would be in serious trouble.\n",
    "If we overfit our training data,\n",
    "there is always the evaluation on test data to keep us honest.\n",
    "But if we overfit the test data, how would we ever know?\n",
    "\n",
    "\n",
    "Thus, we should never rely on the test data for model selection.\n",
    "And yet we cannot rely solely on the training data\n",
    "for model selection either because\n",
    "we cannot estimate the generalization error\n",
    "on the very data that we use to train the model.\n",
    "\n",
    "\n",
    "In practical applications, the picture gets muddier.\n",
    "While ideally we would only touch the test data once,\n",
    "to assess the very best model or to compare\n",
    "a small number of models to each other,\n",
    "real-world test data is seldom discarded after just one use.\n",
    "We can seldom afford a new test set for each round of experiments.\n",
    "\n",
    "The common practice to address this problem\n",
    "is to split our data three ways,\n",
    "incorporating a *validation dataset* (or *validation set*)\n",
    "in addition to the training and test datasets.\n",
    "The result is a murky practice where the boundaries\n",
    "between validation and test data are worryingly ambiguous.\n",
    "Unless explicitly stated otherwise, in the experiments in this book\n",
    "we are really working with what should rightly be called\n",
    "training data and validation data, with no true test sets.\n",
    "Therefore, the accuracy reported in each experiment of the book is really the validation accuracy and not a true test set accuracy.\n",
    "\n",
    "### $K$-Fold Cross-Validation\n",
    "\n",
    "When training data is scarce,\n",
    "we might not even be able to afford to hold out\n",
    "enough data to constitute a proper validation set.\n",
    "One popular solution to this problem is to employ\n",
    "$K$*-fold cross-validation*.\n",
    "Here, the original training data is split into $K$ non-overlapping subsets.\n",
    "Then model training and validation are executed $K$ times,\n",
    "each time training on $K-1$ subsets and validating\n",
    "on a different subset (the one not used for training in that round).\n",
    "Finally, the training and validation errors are estimated\n",
    "by averaging over the results from the $K$ experiments.\n",
    "\n",
    "## Underfitting or Overfitting?\n",
    "\n",
    "When we compare the training and validation errors,\n",
    "we want to be mindful of two common situations.\n",
    "First, we want to watch out for cases\n",
    "when our training error and validation error are both substantial\n",
    "but there is a little gap between them.\n",
    "If the model is unable to reduce the training error,\n",
    "that could mean that our model is too simple\n",
    "(i.e., insufficiently expressive)\n",
    "to capture the pattern that we are trying to model.\n",
    "Moreover, since the *generalization gap*\n",
    "between our training and validation errors is small,\n",
    "we have reason to believe that we could get away with a more complex model.\n",
    "This phenomenon is known as *underfitting*.\n",
    "\n",
    "On the other hand, as we discussed above,\n",
    "we want to watch out for the cases\n",
    "when our training error is significantly lower\n",
    "than our validation error, indicating severe *overfitting*.\n",
    "Note that overfitting is not always a bad thing.\n",
    "With deep learning especially, it is well known\n",
    "that the best predictive models often perform\n",
    "far better on training data than on holdout data.\n",
    "Ultimately, we usually care more about the validation error\n",
    "than about the gap between the training and validation errors.\n",
    "\n",
    "Whether we overfit or underfit can depend\n",
    "both on the complexity of our model\n",
    "and the size of the available training datasets,\n",
    "two topics that we discuss below.\n",
    "\n",
    "### Model Complexity\n",
    "\n",
    "To illustrate some classical intuition\n",
    "about overfitting and model complexity,\n",
    "we give an example using polynomials.\n",
    "Given training data consisting of a single feature $x$\n",
    "and a corresponding real-valued label $y$,\n",
    "we try to find the polynomial of degree $d$\n",
    "\n",
    "$$\\hat{y}= \\sum_{i=0}^d x^i w_i$$\n",
    "\n",
    "to estimate the labels $y$.\n",
    "This is just a linear regression problem\n",
    "where our features are given by the powers of $x$,\n",
    "the model's weights are given by $w_i$,\n",
    "and the bias is given by $w_0$ since $x^0 = 1$ for all $x$.\n",
    "Since this is just a linear regression problem,\n",
    "we can use the squared error as our loss function.\n",
    "\n",
    "\n",
    "A higher-order polynomial function is more complex\n",
    "than a lower-order polynomial function,\n",
    "since the higher-order polynomial has more parameters\n",
    "and the model function's selection range is wider.\n",
    "Fixing the training dataset,\n",
    "higher-order polynomial functions should always\n",
    "achieve lower (at worst, equal) training error\n",
    "relative to lower degree polynomials.\n",
    "In fact, whenever the data examples each have a distinct value of $x$,\n",
    "a polynomial function with degree equal to the number of data examples\n",
    "can fit the training set perfectly.\n",
    "We visualize the relationship between polynomial degree\n",
    "and underfitting vs. overfitting in fig 4.4.1 here https://d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html.\n",
    "\n",
    "\n",
    "### Dataset Size\n",
    "\n",
    "The other big consideration to bear in mind is the dataset size.\n",
    "Fixing our model, the fewer samples we have in the training dataset,\n",
    "the more likely (and more severely) we are to encounter overfitting.\n",
    "As we increase the amount of training data,\n",
    "the generalization error typically decreases.\n",
    "Moreover, in general, more data never hurt.\n",
    "For a fixed task and data distribution,\n",
    "there is typically a relationship between model complexity and dataset size.\n",
    "Given more data, we might profitably attempt to fit a more complex model.\n",
    "Absent sufficient data, simpler models may be more difficult to beat.\n",
    "For many tasks, deep learning only outperforms linear models\n",
    "when many thousands of training examples are available.\n",
    "In part, the current success of deep learning\n",
    "owes to the current abundance of massive datasets\n",
    "due to Internet companies, cheap storage, connected devices,\n",
    "and the broad digitization of the economy.\n",
    "\n",
    "## Polynomial Regression\n",
    "\n",
    "We can now **explore these concepts interactively\n",
    "by fitting polynomials to data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfGGIBO8FhBg",
    "origin_pos": 2,
    "outputId": "c5231fb7-a52f-4bdf-96b2-d0ea1ce7e59e",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: d2l in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from d2l) (1.0.0)\n",
      "Requirement already satisfied: numpy==1.23.5 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from d2l) (1.23.5)\n",
      "Requirement already satisfied: matplotlib==3.7.2 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from d2l) (3.7.2)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from d2l) (0.1.6)\n",
      "Requirement already satisfied: requests==2.31.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from d2l) (2.31.0)\n",
      "Requirement already satisfied: pandas==2.0.3 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from d2l) (2.0.3)\n",
      "Requirement already satisfied: scipy==1.10.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from d2l) (1.10.1)\n",
      "Requirement already satisfied: notebook in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter==1.0.0->d2l) (7.1.3)\n",
      "Requirement already satisfied: qtconsole in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter==1.0.0->d2l) (5.4.2)\n",
      "Requirement already satisfied: jupyter-console in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter==1.0.0->d2l) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter==1.0.0->d2l) (7.10.0)\n",
      "Requirement already satisfied: ipykernel in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter==1.0.0->d2l) (6.25.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter==1.0.0->d2l) (8.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from matplotlib==3.7.2->d2l) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from matplotlib==3.7.2->d2l) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from matplotlib==3.7.2->d2l) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from matplotlib==3.7.2->d2l) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from matplotlib==3.7.2->d2l) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from matplotlib==3.7.2->d2l) (10.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from matplotlib==3.7.2->d2l) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from matplotlib==3.7.2->d2l) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from matplotlib==3.7.2->d2l) (6.1.0)\n",
      "Requirement already satisfied: traitlets in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from matplotlib-inline==0.1.6->d2l) (5.14.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from pandas==2.0.3->d2l) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from pandas==2.0.3->d2l) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from requests==2.31.0->d2l) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from requests==2.31.0->d2l) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from requests==2.31.0->d2l) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from requests==2.31.0->d2l) (2024.2.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib==3.7.2->d2l) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l) (1.16.0)\n",
      "Requirement already satisfied: appnope in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipykernel->jupyter==1.0.0->d2l) (0.1.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipykernel->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipykernel->jupyter==1.0.0->d2l) (8.12.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipykernel->jupyter==1.0.0->d2l) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.5.0)\n",
      "Requirement already satisfied: nest-asyncio in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.5.6)\n",
      "Requirement already satisfied: psutil in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipykernel->jupyter==1.0.0->d2l) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipykernel->jupyter==1.0.0->d2l) (6.3.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipywidgets->jupyter==1.0.0->d2l) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.10)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.36)\n",
      "Requirement already satisfied: pygments in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.15.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (6.0.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.2.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from notebook->jupyter==1.0.0->d2l) (2.14.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from notebook->jupyter==1.0.0->d2l) (2.27.1)\n",
      "Requirement already satisfied: jupyterlab<4.2,>=4.1.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from notebook->jupyter==1.0.0->d2l) (4.1.8)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from notebook->jupyter==1.0.0->d2l) (0.2.4)\n",
      "Requirement already satisfied: ipython-genutils in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from qtconsole->jupyter==1.0.0->d2l) (0.2.0)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from qtconsole->jupyter==1.0.0->d2l) (2.4.1)\n",
      "Requirement already satisfied: webencodings in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
      "Requirement already satisfied: backcall in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
      "Requirement already satisfied: stack-data in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (4.11.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (4.8.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter==1.0.0->d2l) (3.10.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (4.3.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.20.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (0.27.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.2.5)\n",
      "Requirement already satisfied: tomli>=1.2.2 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.0.1)\n",
      "Requirement already satisfied: babel>=2.10 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.9.25)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (4.19.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (2.16.2)\n",
      "Requirement already satisfied: wcwidth in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter==1.0.0->d2l) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.2.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (2023.12.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.10.6)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.7.0)\n",
      "Requirement already satisfied: executing in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: fqdn in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.4)\n",
      "Requirement already satisfied: uri-template in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <ABE0EE74-6D97-3B8C-B690-C44754774FBC> /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <EA4AB737-8320-3563-ACC4-78D44C0573BA> /Users/jaschob/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "! pip install d2l\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wvu0ZZ6GFhBh",
    "origin_pos": 4
   },
   "source": [
    "### Generating the Dataset\n",
    "\n",
    "First we need data. Given $x$, we will **use the following cubic polynomial to generate the labels** on training and test data:\n",
    "\n",
    "**$$y = 5 + 1.2x - 3.4\\frac{x^2}{2!} + 5.6 \\frac{x^3}{3!} + \\epsilon \\text{ where }\n",
    "\\epsilon \\sim \\mathcal{N}(0, 0.1^2).$$**\n",
    "\n",
    "The noise term $\\epsilon$ obeys a normal distribution\n",
    "with a mean of 0 and a standard deviation of 0.1.\n",
    "For optimization, we typically want to avoid\n",
    "very large values of gradients or losses.\n",
    "This is why the *features*\n",
    "are rescaled from $x^i$ to $\\frac{x^i}{i!}$.\n",
    "It allows us to avoid very large values for large exponents $i$.\n",
    "We will synthesize 100 samples each for the training set and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yfhRZ7COFhBi",
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "max_degree = 20  # Maximum degree of the polynomial\n",
    "n_train, n_test = 100, 100  # Training and test dataset sizes\n",
    "true_w = np.zeros(max_degree)  # Allocate lots of empty space\n",
    "true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n",
    "\n",
    "features = np.random.normal(size=(n_train + n_test, 1))\n",
    "np.random.shuffle(features)\n",
    "poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\n",
    "for i in range(max_degree):\n",
    "    poly_features[:, i] /= math.gamma(i + 1)  # `gamma(n)` = (n-1)!\n",
    "# Shape of `labels`: (`n_train` + `n_test`,)\n",
    "labels = np.dot(poly_features, true_w)\n",
    "labels += np.random.normal(scale=0.1, size=labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkbbXtm5FhBi",
    "origin_pos": 6
   },
   "source": [
    "Again, monomials stored in `poly_features`\n",
    "are rescaled by the gamma function,\n",
    "where $\\Gamma(n)=(n-1)!$.\n",
    "**Take a look at the first 2 samples** from the generated dataset.\n",
    "The value 1 is technically a feature,\n",
    "namely the constant feature corresponding to the bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uTty-2lfFhBj",
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# Convert from NumPy ndarrays to tensors\n",
    "true_w, features, poly_features, labels = [\n",
    "    torch.tensor(x, dtype=torch.float32)\n",
    "    for x in [true_w, features, poly_features, labels]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fOZp6IwzFhBk",
    "origin_pos": 8,
    "outputId": "9efe8116-7e20-4e18-f901-8640843925ec",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8529],\n",
       "         [-0.3562]]),\n",
       " tensor([[ 1.0000e+00, -8.5291e-01,  3.6373e-01, -1.0341e-01,  2.2050e-02,\n",
       "          -3.7613e-03,  5.3468e-04, -6.5148e-05,  6.9457e-06, -6.5823e-07,\n",
       "           5.6141e-08, -4.3531e-09,  3.0940e-10, -2.0299e-11,  1.2367e-12,\n",
       "          -7.0319e-14,  3.7485e-15, -1.8807e-16,  8.9113e-18, -4.0003e-19],\n",
       "         [ 1.0000e+00, -3.5617e-01,  6.3429e-02, -7.5305e-03,  6.7054e-04,\n",
       "          -4.7765e-05,  2.8354e-06, -1.4427e-07,  6.4231e-09, -2.5419e-10,\n",
       "           9.0536e-12, -2.9315e-13,  8.7009e-15, -2.3839e-16,  6.0647e-18,\n",
       "          -1.4400e-19,  3.2056e-21, -6.7162e-23,  1.3290e-24, -2.4912e-26]]),\n",
       " tensor([2.2245, 4.4499]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:2], poly_features[:2, :], labels[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1trB-InFhBl",
    "origin_pos": 9
   },
   "source": [
    "### Training and Testing the Model\n",
    "\n",
    "Let us first **implement a function to evaluate the loss on a given dataset**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qVXxHobRFhBl",
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_loss(net, data_iter, loss): \n",
    "    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\n",
    "    metric = d2l.Accumulator(2)  # Sum of losses, no. of examples\n",
    "    for X, y in data_iter:\n",
    "        out = net(X)\n",
    "        y = y.reshape(out.shape)\n",
    "        l = loss(out, y)\n",
    "        metric.add(l.sum(), l.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67C4KtwnFhBm",
    "origin_pos": 12
   },
   "source": [
    "Now **define the training function**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p5OVhsHBFhBm",
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def train(train_features, test_features, train_labels, test_labels,\n",
    "          num_epochs=400):\n",
    "    loss = nn.MSELoss()\n",
    "    input_shape = train_features.shape[-1]\n",
    "    # Switch off the bias since we already catered for it in the polynomial\n",
    "    # features\n",
    "    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\n",
    "    batch_size = min(10, train_labels.shape[0])\n",
    "    train_iter = d2l.load_array((train_features, train_labels.reshape(-1, 1)),\n",
    "                                batch_size)\n",
    "    test_iter = d2l.load_array((test_features, test_labels.reshape(-1, 1)),\n",
    "                               batch_size, is_train=False)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',\n",
    "                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],\n",
    "                            legend=['train', 'test'])\n",
    "    for epoch in range(num_epochs):\n",
    "        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\n",
    "        if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "            animator.add(epoch + 1, (evaluate_loss(\n",
    "                net, train_iter, loss), evaluate_loss(net, test_iter, loss)))\n",
    "    print('weight:', net[0].weight.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5suWgrUhFhBn",
    "origin_pos": 16
   },
   "source": [
    "### **Third-Order Polynomial Function Fitting (Normal)**\n",
    "\n",
    "We will begin by first using a third-order polynomial function, which is the same order as that of the data generation function.\n",
    "The results show that this model's training and test losses can be both effectively reduced.\n",
    "The learned model parameters are also close\n",
    "to the true values $w = [5, 1.2, -3.4, 5.6]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "DRGiqmA3FhBn",
    "origin_pos": 17,
    "outputId": "3d4935e6-f2cd-418b-ac51-eab9d504f623",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'd2l.torch' has no attribute 'train_epoch_ch3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Pick the first four dimensions, i.e., 1, x, x^2/2!, x^3/3! from the\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# polynomial features\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoly_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_features, test_features, train_labels, test_labels, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m animator \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mAnimator(xlabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m, ylabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, yscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m                         xlim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, num_epochs], ylim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1e-3\u001b[39m, \u001b[38;5;241m1e2\u001b[39m],\n\u001b[1;32m     16\u001b[0m                         legend\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 18\u001b[0m     \u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch_ch3\u001b[49m(net, train_iter, loss, trainer)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m         animator\u001b[38;5;241m.\u001b[39madd(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, (evaluate_loss(\n\u001b[1;32m     21\u001b[0m             net, train_iter, loss), evaluate_loss(net, test_iter, loss)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'd2l.torch' has no attribute 'train_epoch_ch3'"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"240.554688pt\" height=\"173.477344pt\" viewBox=\"0 0 240.554688 173.477344\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-05-06T18:26:15.463579</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 173.477344 \n",
       "L 240.554688 173.477344 \n",
       "L 240.554688 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 225.403125 149.599219 \n",
       "L 225.403125 10.999219 \n",
       "L 30.103125 10.999219 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m00b96e7b57\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00b96e7b57\" x=\"30.103125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(22.151563 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00b96e7b57\" x=\"69.163125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(61.211563 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00b96e7b57\" x=\"108.223125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(100.271563 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00b96e7b57\" x=\"147.283125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(139.331563 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00b96e7b57\" x=\"186.343125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(178.391563 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00b96e7b57\" x=\"225.403125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(217.451563 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m735133a34b\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m735133a34b\" x=\"30.103125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(7.2 153.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m735133a34b\" x=\"30.103125\" y=\"121.879219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(7.2 125.678438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m735133a34b\" x=\"30.103125\" y=\"94.159219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(7.2 97.958438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m735133a34b\" x=\"30.103125\" y=\"66.439219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(7.2 70.238438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m735133a34b\" x=\"30.103125\" y=\"38.719219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(7.2 42.518438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m735133a34b\" x=\"30.103125\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(7.2 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 30.103125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 225.403125 149.599219 \n",
       "L 225.403125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 225.403125 149.599219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 10.999219 \n",
       "L 225.403125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pick the first four dimensions, i.e., 1, x, x^2/2!, x^3/3! from the\n",
    "# polynomial features\n",
    "train(poly_features[:n_train, :4], poly_features[n_train:, :4],\n",
    "      labels[:n_train], labels[n_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tT4BBEgtFhBo",
    "origin_pos": 18
   },
   "source": [
    "### **Linear Function Fitting (Underfitting)**\n",
    "\n",
    "Let us take another look at linear function fitting.\n",
    "After the decline in early epochs,\n",
    "it becomes difficult to further decrease\n",
    "this model's training loss.\n",
    "After the last epoch iteration has been completed,\n",
    "the training loss is still high.\n",
    "When used to fit nonlinear patterns\n",
    "(like the third-order polynomial function here)\n",
    "linear models are liable to underfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "iFNOPPBVFhBo",
    "origin_pos": 19,
    "outputId": "56333429-2840-47c6-fbfd-687dc791df3f",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# Pick the first two dimensions, i.e., 1, x, from the polynomial features\n",
    "train(poly_features[:n_train, :2], poly_features[n_train:, :2],\n",
    "      labels[:n_train], labels[n_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmmOk7xKFhBp",
    "origin_pos": 20
   },
   "source": [
    "### **Higher-Order Polynomial Function Fitting  (Overfitting)**\n",
    "\n",
    "Now let us try to train the model\n",
    "using a polynomial of too high degree.\n",
    "Here, there are insufficient data to learn that\n",
    "the higher-degree coefficients should have values close to zero.\n",
    "As a result, our overly-complex model\n",
    "is so susceptible that it is being influenced\n",
    "by noise in the training data.\n",
    "Though the training loss can be effectively reduced,\n",
    "the test loss is still much higher.\n",
    "It shows that\n",
    "the complex model overfits the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "XWqqf6zlFhBp",
    "origin_pos": 21,
    "outputId": "49e014c5-7e90-4c27-dc74-61e93c5536d3",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# Pick all the dimensions from the polynomial features\n",
    "train(poly_features[:n_train, :], poly_features[n_train:, :],\n",
    "      labels[:n_train], labels[n_train:], num_epochs=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io9LwIQ4FhBq",
    "origin_pos": 22
   },
   "source": [
    "In the subsequent sections, we will continue\n",
    "to discuss overfitting problems\n",
    "and methods for dealing with them,\n",
    "such as weight decay and dropout.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Since the generalization error cannot be estimated based on the training error, simply minimizing the training error will not necessarily mean a reduction in the generalization error. Machine learning models need to be careful to safeguard against overfitting so as to minimize the generalization error.\n",
    "* A validation set can be used for model selection, provided that it is not used too liberally.\n",
    "* Underfitting means that a model is not able to reduce the training error. When training error is much lower than validation error, there is overfitting.\n",
    "* We should choose an appropriately complex model and avoid using insufficient training samples.\n",
    "\n",
    "\n",
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9kk47bbL-3O"
   },
   "source": [
    "1. Name some ideas on how you could perform model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOOgVok7FhBq",
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/97)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "underfit-overfit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
